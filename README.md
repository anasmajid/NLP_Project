## Directory Structure

 * [requirements.txt](./requirements.txt)
 * [README.md](./README.md)
 * [backend](./backend)
   * [joint.py](./backend/joint.py)
   * [out.out](./backend/out.out)
   * [sarcasm.py](./backend/sarcasm.py)
   * [subjectivity.py](./backend/subjectivity.py)
   * [util.py](./backend/util.py)
   * [sentiment.py](./backend/sentiment.py)
   * [run.py](./backend/run.py)
   * [\_\_pycache\_\_](./backend/__pycache__)
   * [model](./backend/model)
      * [joint.h5](./backend/model/joint.h5)
      * [sentiment.h5](./backend/model/sentiment.h5)
      * [subjectivity.h5](./backend/model/subjectivity.h5)
      * [sarcasm.h5](./backend/model/sarcasm.h5)
 * [data](./data)
   * [PennsylvaniaMidterms11Aug11Oct2.xlsx](./data/PennsylvaniaMidterms11Aug11Oct2.xlsx)
 * [notebooks](./notebooks)
   * [Sarcasm.ipynb](./notebooks/Sarcasm.ipynb)
   * [base_models_and_joint_model.ipynb](./notebooks/base_models_and_joint_model.ipynb)
   * [ensemble.ipynb](./notebooks/ensemble.ipynb)
   * [Baseline1.ipynb](./notebooks/Baseline1.ipynb)
 * [results](./results)
   * [automatic_prediction.csv](./results/automatic_prediction.csv)
 * [scraping](./scraping)
   * [tweets_scraper.py](./scraping/tweets_scraper.py)
   * [NLPScraping.ipynb](./scraping/NLPScraping.ipynb)
 * [ui](./ui)
   * [tweets_scraper.py](./ui/tweets_scraper.py)
   * [main.py](./ui/main.py)
   * [wrapper.py](./ui/wrapper.py)
   * [\_\_pycache\_\_](./ui/__pycache__)


## Steps to run the UI:

1.  Download dependencies from requirements.txt

2.  Ensure that the 4 models are placed in the backend/model folder,
    A copy of the models can also be found here:
    https://drive.google.com/drive/folders/1SNqxib7TuLN9v92l0yk9SstHJLs7UQPG?usp=sharing

3.  Navigate to the ui folder and run main.py

4.  See UI at http://127.0.0.1:7860


## Steps to run the backend without the UI:

1.  Download dependencies from requirements.txt

2.  Navigate to the backend folder and run run.py

3.  Or use the following command on Linux:
    nohup python3 run.py > out.out


## Summary of folder contents:
backend (Contains code to run models through the UI)
    - model (Place the following models in this folder)
notebooks (Contains preprocessing, training, testing code for models - including baseline models)
results (Contains output generated by each of the models - See Additional Notes #3)
scraping (Contains scraping code)
ui (Contains frontend UI, and middleware code, including script used for live scraping)


## Additional notes:

1.  Model takes in a list of strings

2.  model.score will return a list. In case of a single input, just use returned_result[0]

3. results/automatic_prediction.csv contains some tweets that will output an error, so all results for those tweets have been set to __-1__. These tweets are not in english language or have lots of meaningless strings.