## Steps to run the UI:

1.  Download dependencies from requirements.txt

2.  Ensure that the 4 models are placed in the backend/model folder,
    A copy of the models can also be found here:
    https://drive.google.com/drive/folders/1SNqxib7TuLN9v92l0yk9SstHJLs7UQPG?usp=sharing

3.  Navigate to the ui folder and run main.py

4.  See UI at http://127.0.0.1:7860


## Steps to run the backend without the UI:

1.  Download dependencies from requirements.txt

2.  Navigate to the backend folder and run run.py

3.  Or use the following command on Linux:
    nohup python3 run.py > out.out


## Directory Structure
├───
│   README.md
│   requirements.txt
│
├───backend (Contains code to run models through the UI)
│   │   joint.py
│   │   out.out
│   │   run.py
│   │   sarcasm.py
│   │   sentiment.py
│   │   subjectivity.py
│   │   util.py
│   │
│   ├───model (Place the following models in this folder)
│   │       joint.h5
│   │       sarcasm.h5
│   │       sentiment.h5
│   │       subjectivity.h5
│   │
│   └───__pycache__
│
├───notebooks (Contains preprocessing, training, testing code for models - including baseline models)
│       Baseline1.ipynb
│       base_models_and_joint_model.ipynb
│       ensemble.ipynb
│       Sarcasm.ipynb
│
├───results (Contains output generated by each of the models - See Additional Notes #3)
│       automatic_prediction.csv
│
├───scraping (Contains scraping code)
│       NLPScraping.ipynb
│       tweets_scraper.py
│
└───ui (Contains frontend UI, and middleware code, including script used for live scraping)
    │   main.py
    │   tweets_scraper.py
    │   wrapper.py
    │
    └───__pycache__


## Additional notes:

1.  Model takes in a list of strings

2.  model.score will return a list. In case of a single input, just use returned_result[0]

3. results/automatic_prediction.csv contains some tweets that will output an error, so all results for those tweets have been set to -1. These tweets are not in english language or have lots of meaningless strings.